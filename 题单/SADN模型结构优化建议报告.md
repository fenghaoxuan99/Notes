
# SADN模型结构优化建议报告

## 1. SADN模型概述
SADN (Spatial-Angular-Decorrelation Network) 通过空间-角度交互模块解耦光场图像的视角信息和空间信息，再经特征融合后使用层次注意力压缩网络进行编码。当前模型在Spatial-Angular Interaction Block中，使用膨胀卷积和跨视角卷积提取信息，并以Cheng等人的注意力自适应网络为基础完成主干编码。该结构已取得领先压缩性能，但仍有改进空间。以下基于2023–2025年最新研究，提出多方面优化建议，以进一步提升压缩率和重建质量（PSNR/MS-SSIM）。

## 2. 更高效的上下文建模
*   **多尺度渐进式编码：** 参考最近提出的“分层渐进上下文模型 (HPCM)”。该方法将潜变量划分为多尺度子集，先编码低分辨率层，再逐级细化高分辨率层，从而用更大感受野捕获远程依赖，有效提升上下文预测精度。在SADN中可仿照此思路，对g_a/g_s输出的潜变量进行层次划分，先粗编码全局低频，再局部细致补充高频细节，进而提高上下文估计能力。
*   **因果卷积与轻量注意力：** 在自回归上下文模型中，引入因果（Mask）卷积，如Minnen等工作中使用的方式，可以确保像素依赖性仅来自已编码区域。此类因果卷积可以结合轻量级注意力模块（如深度可分离卷积后的通道注意力），在扩展上下文范围的同时保持计算效率。此外，研究表明局部注意力优于全局注意力来重建纹理。因此，可在上下文模型中采用局部注意机制（参见下文）以增强对邻域细节的建模。
*   **分组上下文策略：** 借鉴“锚点预测”（Checkerboard扫描）思路
   
    ，可将潜变量分为多个组（如隔像素、通道分块），先解码一部分作为锚点，再利用其上下文信息解码其他部分。这种交错式解码可以更高效地捕获空间依赖关系，从而进一步优化熵估计。

## 3. 非Transformer注意力模块
*   **窗口（局部）注意力：** 采用类似Zou等CVPR2022提出的窗口注意力模块。该模块在局部窗口内计算自注意力，有效捕捉临近像素之间的细节关联。窗口注意力可插入到SADN的InterNet或主干特征流中，增强对高频纹理的关注，如研究所示：局部注意力对于纹理重建尤为关键。
*   **轴向注意力：** 可在空间或角度维度上分别施加注意力机制（Axial Attention），先沿一个方向聚焦，再沿另一方向聚焦，从而高效捕捉长程依赖。虽然Transformer思想，该操作仅在一维上进行注意力，计算量相对较低。此思路已被用于医学图像处理（见相关Axial Attention研究），可借鉴设计将其引入视角或空间轴向。
*   **通道注意力：** 例如SE、ECA、GCT等模块可用于提升通道层次的表示能力。高效通道注意力（如GCT）通过学习全局上下文与通道激活之间的高斯映射关系，在不增加显著计算的前提下提高了性能。在SADN中，可在g_a/g_s或h_a/h_s的特征通道上加入此类通道注意力模块，强化有意义特征通道。
*   **轻量化注意力设计：** 可参考Efficient Attention等轻量框架，设计计算量较低的注意力单元。例如，使用1×1卷积实现类似空间注意力或组间注意力，而非全局多头机制。这类轻量注意力可用于InterNet或特征融合模块，在复杂度受限的情况下提升性能。

## 4. 改进的熵模型与先验建模
*   **多尺度联合熵建模：** 结合分层上下文，可设计多尺度熵模型。可在超先验h_a/h_s模块中增加多级隐藏变量，将潜变量按尺度分层编码；或在条件熵模型中同时考虑不同分辨率的上下文，例如同时使用粗尺度和细尺度预测子位。多尺度策略能更精细地拟合潜变量分布，进一步降低冗余。
*   **高斯混合/广义高斯分布：** 在潜变量熵模型中，采用高斯混合（GMM）或广义高斯(GGM)分布来替代单一高斯。Cheng等已提出离散高斯混合模型来提升分布拟合度，而广义高斯模型仅增加一个形状参数就能大幅提高灵活度。实验表明，GGM在多种模型中优于单高斯和高斯混合，可以将其集成到SADN的先验网络中以更准确地估计概率分布。
*   **可变形上下文建模：** 引入可变形卷积等机制，动态调整上下文区域。例如在特征图上使用可变形采样，根据内容结构选择邻域像素。此技术在语义分割中已有应用，可用于上下文提取步骤，以适应物体边缘和纹理等非均匀特征，从而提升熵估计精度。
*   **联合概率模型：** 设计更丰富的条件概率模型，例如同时输出均值、方差及混合系数。可参考Barlow Twins或Normalizing Flow思想，将超先验输出多个参数，以组合或混合分布形式对潜变量进行建模，从而提升自适应性。

## 5. 残差预测模块优化
*   **空间-角度交互增强：** 加强残差预测网络中的空间-角度信息交互。可以在残差预测阶段加入视差估计或几何引导。例如，可参考基于EPI恢复的方法，将视差平面图(EPI)视作低秩结构，通过少量视角恢复完整EPI，剩余误差再编码。该方法利用光场的内在相似性，达到更优压缩效果。SADN的InterNet可引入类似的模块，通过EPI网络或位移网络先生成初步预测，再学习残差。
*   **引导式几何预测：** 采用多流几何重建策略。例如Zhang等提出的两阶段重建网络：先利用已编码的稀疏视图和深度信息进行几何引导的粗略重建，然后再通过纹理融合细化图像。SADN可借鉴，在残差模块中添加几何先验（如深度或视差图），指导残差网络更准确地预测图像差值，尤其在具有复杂遮挡或视差变化的区域。
*   **多参考残差预测：** 结合多个相邻视角的信息作为参考，预测目标视角图像的残差。例如，利用四角或邻域SAI生成中间视图后，将其与真实图像差值作为残差编码；或者在残差网络中并行输入多个视角，通过融合层进行联合预测。此思路类似视频或多视角压缩中的多参考预测，能充分利用角度冗余降低残差能量。

## 6. 解码器结构轻量化
*   **非对称轻量设计：** 参考AsymLLIC等工作采用不对称编码器-解码器架构
   
    具体做法是将更复杂的运算放在编码器端，精简解码器结构。例如通过逐步将原有的重卷积、逆卷积替换为简单的反卷积或子像素卷积(像素重排)，并引入深度可分离卷积等高效块。Wang等工作成功将解码器计算量削减至51.47 GMACs，参数量仅19.65M，同时保持了接近VVC的性能

    对SADN而言，可采用类似训练策略：先用全容量解码器预训练，再逐层替换为轻量模块并微调，最终获得轻量级解码器
  
    
*   **模块化高效设计：** 对解码器中的每个模块进行优化，例如将大核卷积拆分（如先做竖直再水平卷积），减少参数。对于上采样，可使用亚像素卷积或邻近插值配合1×1卷积替代大尺度转置卷积。利用可逆神经网络结构或切线块（coupling layers）也可降低逆变换开销。
*   **剪枝与知识蒸馏：** 对训练好的解码器使用剪枝、低秩分解等技术减小模型尺寸，或通过蒸馏让小模型学习大模型输出，可在保留PSNR/MS-SSIM的同时降低解码计算量，利于嵌入式应用。

## 7. 模块替换可行性与落地性分析
*   **InterNet（空间-角度交互）：** 该块负责视角与空间信息融合，可替换为更强的交互单元。例如可在SFE/AFE之后增加交叉注意力或门控机制，实现空间特征对角度特征的引导融合。引入深度预测或视差图作为额外输入，也可提升交互效果。多流几何重建网络表明，通过并行分支捕获几何信息有助于重建质量。由于InterNet核心思想是保留并融合空间-角度信息，此处替换难度适中，关键在于保持光场一致性。
*   **g_a/g_s（分析/合成变换）：** 这是主干特征编码器，可用更高效的卷积结构替换。解码器端（g_s）尤可借鉴AsymLLIC中的轻量化模块
    
    。例如将深度可分离卷积、组卷积等集成到编码/解码网络，或在卷积层后加入通道注意力以补偿精度。由于这些网络可通过训练得到参数，只要新结构容量充足即可替换，因此具有较高的可行性。
*   **h_a/h_s（超先验）：** 超先验网络主要估计潜变量分布参数，可以加入上述GMM/GGM等增强模型。广义高斯分布仅增加一个形状参数，就可直接融入潜在分布预测。还可设计多级超先验（如级联隐变量）或分支结构输出多组参数，提升预测精度。超先验模块往往计算量较低，引入新建模方法带来的复杂度上升有限，可行性较高。
*   **落地性：** 上述优化大多属于网络结构层面的替换和增强，需要重新训练或微调模型。由于不改变端到端编解码流程，仅更新可学习模块，改进方案具有可实施性。参考开源的CompressAI框架和最新LIC项目，可作为实现上述建议的基础环境。总体而言，这些优化将在保证SADN框架核心特点的前提下，有望显著提升模型在PSNR/MS-SSIM下的率失真表现。

## 参考文献（部分相关工作）
窗口注意力模型；层次渐进上下文模型；通用高斯模型；EPI恢复网络；几何引导视图重建；轻量化非对称解码策略；以及Cheng等GMM熵模型等。以上工作提供了本报告建议中的技术思路和验证依据。
