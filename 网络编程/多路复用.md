
# I/O 多路复用：select/poll/epoll

## 基本 Socket 模型
*   **目的：** 跨主机进程间通信。
*   **核心概念：** 通信双方各自创建 Socket（相当于通信端点），绑定 IP 地址和端口。
    *   **端口绑定：** 内核通过端口号将 TCP/UDP 报文传递给正确的应用程序。
    *   **IP 地址绑定：** 指定接收哪个网卡的数据。
*   **TCP 服务端流程：**
    1.  `socket()`：创建 Socket。
    2.  `bind()`：绑定 IP 和端口。
    3.  `listen()`：进入监听状态（`LISTEN`）。
    4.  `accept()`：阻塞等待客户端连接，返回已连接 Socket。
*   **TCP 客户端流程：**
    1.  `socket()`：创建 Socket。
    2.  `connect()`：指定服务端 IP 和端口发起连接（触发三次握手）。
*   **内核队列：**
    *   **半连接队列 (`SYN_RCVD`)：** 未完成三次握手的连接。
    *   **全连接队列 (`ESTABLISHED`)：** 已完成三次握手的连接。
*   **Socket 类型：**
    *   **监听 Socket：** 用于接收新连接。
    *   **已连接 Socket：** 用于与特定客户端传输数据。
*   **数据传输：** 使用 `read()`/`write()` 在已连接 Socket 上进行。
*   **Linux 实现：** Socket 被视为文件，拥有文件描述符 (fd)。内核 Socket 结构包含发送队列和接收队列（存储 `sk_buff` 结构表示各层数据包）。

## 服务更多用户的挑战 (C10K)
*   **问题：** 基本阻塞模型只能一对一通信，效率低下。
*   **理论最大连接数：** 客户端 IP 数 × 客户端端口数（IPv4 约 2^48）。
*   **实际限制：**
    *   文件描述符限制 (可通过 `ulimit` 调整)。
    *   系统内存 (每个连接占用内存)。
*   **C10K 目标：** 单机同时处理 1 万个并发请求。低效的 I/O 模型会阻碍达成此目标。

## 传统解决方案及其瓶颈
1.  **多进程模型：**
    *   为每个客户端连接 `fork()` 一个子进程处理。
    *   **优点：** 逻辑清晰。
    *   **缺点：** 进程创建/销毁开销大，上下文切换代价高（涉及用户和内核资源），资源占用多。难以支撑 C10K。
2.  **多线程模型：**
    *   为每个客户端连接创建线程，或使用线程池 + 任务队列。
    *   **优点：** 线程共享进程资源，上下文切换开销小于进程。
    *   **缺点：** 频繁创建/销毁线程仍有开销（线程池缓解），大量线程的调度和内存消耗仍是瓶颈（万级连接困难）。需处理线程同步（如队列加锁）。

## I/O 多路复用
*   **核心思想：** **一个进程** 管理 **多个 Socket 连接**。通过高效方式检查多个 Socket 的状态，只处理真正有 I/O 事件的连接。
*   **工作原理类比：** 时分复用。进程快速轮询处理多个请求。
*   **系统调用：** `select`, `poll`, `epoll`。用户通过一次系统调用获取多个就绪事件。

### select/poll
*   **流程：**
    1.  用户将需要监听的 **所有 Socket fd 集合** 传入内核。
    2.  内核 **线性遍历** 该集合，检查是否有事件（可读/可写）。
    3.  内核将有事件的 Socket 标记为就绪。
    4.  内核将 **整个 fd 集合（包含未就绪的）** 拷贝回用户态。
    5.  用户程序 **再次线性遍历** 整个集合，找出就绪的 Socket 进行处理。
*   **主要问题：**
    *   **两次遍历：** 内核遍历 + 用户遍历 (O(n) 复杂度)。
    *   **两次拷贝：** 用户态->内核态，内核态->用户态 (集合很大时开销大)。
    *   **fd 数量限制：**
        *   `select`：使用固定大小位图 (`fd_set`)，默认最大 1024 (`FD_SETSIZE`)。
        *   `poll`：使用动态数组（链表），突破 `select` 限制，但仍有系统级上限。
*   **瓶颈：** 随着连接数增加，遍历和拷贝开销呈指数级增长，难以应对 C10K。

### epoll
*   **设计目标：** 高效解决 select/poll 的瓶颈，支撑 C10K/C100K。
*   **核心优化：**
    *   **1. 红黑树管理 fd：**
        *   使用 `epoll_create` 创建 epoll 对象 (`epfd`)。
        *   使用 `epoll_ctl` 将待监听的 Socket fd **增量式** 添加到内核的 **红黑树** 中。增删改查效率高 (O(log n))。
        *   **避免：** 每次调用都传递整个 fd 集合，显著减少内核/用户态拷贝和内存分配。
    *   **2. 就绪链表记录事件：**
        *   当某个 Socket 有事件发生时，内核通过 **回调函数** 将其 fd 加入一个 **就绪事件链表**。
        *   用户调用 `epoll_wait` 时，内核 **仅返回** 该就绪链表中的 fd 及其事件 (数量通常远小于总 fd 数)。
        *   **避免：** 内核和用户态的无谓遍历 (O(1) 获取就绪事件。
*   **高效性：** 监听 Socket 数量增大时，性能不会显著下降。可监听上限为系统最大文件描述符数。
*   **事件触发模式：**
    *   **水平触发 (LT - Level-Triggered, 默认)：** 只要 Socket 缓冲区有数据可读或空间可写，`epoll_wait` 就会持续报告该事件。用户可多次读取/写入。
    *   **边缘触发 (ET - Edge-Triggered)：** 仅当 Socket 状态**发生变化时**（如从无数据到有数据）报告一次事件。用户必须在这次事件中**非阻塞地一次性**读取/写入尽可能多的数据，否则可能丢失后续事件通知。
    *   **ET 效率：** 通常更高，减少 `epoll_wait` 调用次数。
    *   **ET 要求：** **必须搭配非阻塞 I/O**。读写时若数据耗尽/空间满，系统调用 (read/write) 会返回 `EAGAIN`/`EWOULDBLOCK` 错误而非阻塞，避免进程卡住。
*   **重要实践：** 使用 I/O 多路复用 (`select`/`poll`/`epoll`) 时，**强烈建议搭配非阻塞 I/O**。因为 API 返回“就绪”后，调用 `read`/`write` 仍可能因数据校验错误等原因短暂阻塞，非阻塞 I/O 可避免此情况。

## 总结
1.  **基础模型：** 阻塞 I/O，一对一通信。
2.  **改进尝试：**
    *   **多进程/多线程：** 实现一对多，但进程/线程本身成为资源瓶颈（创建、销毁、切换、内存），难以应对高并发 (C10K)。
3.  **I/O 多路复用：** 一个进程管理多个连接。Linux 提供 `select`, `poll`, `epoll`。
    *   **select/poll：** 使用线性结构存储 fd 集合。需两次遍历（内核+用户）和两次拷贝（传入+传回集合），效率低 (O(n))。
    *   **epoll：** 解决 C10K 利器。
        *   红黑树管理 fd (O(log n) 增删改查)，避免每次传递完整集合。
        *   就绪链表记录事件 (O(1) 获取就绪 fd)，避免无效遍历。
        *   支持高效的边缘触发 (ET) 模式 (需搭配非阻塞 I/O)。
4.  **通用建议：** I/O 多路复用 API 应配合非阻塞 I/O 使用。